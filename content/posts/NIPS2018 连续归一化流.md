---
title: NIPS2018 连续归一化流
date: 2025-03-29T13:54:40
slug: NIPS2018
tags:
  - "#Math"
categories:
  - 数学基础
description: 对NIPS2018中的连续型归一化流的推导和理解
summary: 主要内容为连续化神经网络和优化的连续型归一化流的计算和理解
cover:
  image:
draft: false
share: true
---

## 常微分方程

这一节先把常微分方程（ODE）的直觉捋清楚：它描述的是“一个状态量如何随时间连续演化”。之后再把“网络的层”看成“时间上的离散步”，就能自然理解神经常微分方程（Neural ODE）以及连续归一化流（CNF）里那些看起来很“物理”的公式。

一般的 ODE 可以写成：

$$
\frac{d y(t)}{d t} = g\left(y(t), t\right),\quad y(t_0)=y_0
$$

给定初值 $y(t_0)$ 后，很多时候我们只关心另一个时刻 $t_1$ 的状态 $y(t_1)$。多数 ODE 没有解析解，于是工程里通常调用数值积分器（Euler、Runge–Kutta 等），用很多小步把 $t_0$ 推到 $t_1$：

$$
y_{k+1} \approx y_k + \Delta t\; g(y_k, t_k)
$$

把视角切到神经网络：深层网络本质上是很多变换的复合。比如把隐藏状态记为 $h$，一层对应一次更新 $h_{k+1}=\Phi_k(h_k)$。如果每一层的更新都只是在做一个“小增量”，那么它就很像上面数值积分里“前进一步”的形式，这也是“把网络连续化”的出发点。

另外一个常被提到的点是内存：普通反向传播需要缓存每一层激活值；而在 Neural ODE 的框架下，可以用“伴随法（adjoint）”从终点反推梯度，把存储开销从“随层数线性增长”变成“更接近常数”，代价是会增加一些计算量（比如要再解一次 ODE）。

## 从残差网络到微分方程
残差网络（ResNet）最适合用来建立“离散层”与“连续时间”之间的联系。先看一个典型残差块（下图）：
![](https://raw.githubusercontent.com/powerli2002/project-img/main/myblog/20250329140847800.png)

把它抽象成一个更新方程：

$$
h_{t+1} = h_t + f(h_t,\theta_t)
$$

也就是说，网络模块 $f$ 并不是直接“输出下一层状态”，而是输出一个**残差增量**。进一步，如果我们显式写出步长 $\Delta t$，可以把它写成差商的味道：

$$
\frac{h_{t+\Delta t}-h_t}{\Delta t} = f(h_t,t,\theta)
$$

当 $\Delta t \to 0$ 时，左边就变成了导数，于是得到一个连续时间的动力系统：

$$
\frac{d h(t)}{d t} = f(h(t), t,\theta)
$$
直觉上可以把 $t$ 看成“网络深度的连续化版本”，把 $h(t)$ 看成“随深度连续演化的隐藏状态”。此时前向传播不再是“堆很多层”，而是“把 ODE 从 $t_0$ 积分到 $t_1$”。

把上式从 $t_0$ 积分到 $t_1$，得到：

$$
\frac{d h ( t )}{d t} = f ( h ( t ), t, \theta)
$$

$$
\int_{t_{0}}^{t_{1}} d h ( t ) = \int_{t_{0}}^{t_{1}} f ( h ( t ), t, \theta) d t
$$

$$
h ( t_{1} ) = h ( t_{0} )+\int_{t_{0}}^{t_{1}} f ( h ( t ), t, \theta) d t
$$
实际实现时，我们通常不手推积分，而是把 $f$ 当成一个黑盒向量场，交给 ODE solver 去输出 $h(t_1)$（以及训练时需要的梯度）。

## **连续型的归一化流**

### 流生成模型
归一化流（Normalizing Flow）是一类“既能采样、又能精确算密度”的生成模型。它从一个容易处理的基分布出发（例如高斯），通过一串可逆变换把样本推到目标空间。

如果 $z_0\sim p_0(z_0)$，经过可逆变换 $z_1=f_1(z_0),\ldots,z_K=f_K(z_{K-1})$ 得到 $z_K$，那么对数密度可以用变量代换公式计算。

先从一维直觉开始：若 $z\sim \pi(z)$，令 $x=f(z)$ 且 $f$ 可逆，那么新变量 $x$ 的密度满足：

$$
\int p(x)dx=\int\pi(z)dz=1\text{ ; Definition of probability distribution.}
$$

$$
p(x)=\pi(z)\left|\frac{dz}{dx}\right|=\pi(f^{-1}(x))\left|\frac{df^{-1}}{dx}\right|=\pi(f^{-1}(x))|(f^{-1})^{\prime}(x)|
$$

这里的绝对值反映了变换的“伸缩比例”：拉伸空间会稀释密度，压缩空间会增加密度。这个思想在高维里由雅可比行列式来刻画体积变化。

对多维变量 $\mathbf{z}$ 和 $\mathbf{x}=f(\mathbf{z})$，变量代换公式写成：

$$
\mathbf{z}\sim\pi(\mathbf{z}),\mathbf{x}=f(\mathbf{z}),\mathbf{z}=f^{-1}(\mathbf{x})
$$

$$
p(\mathbf{x})=\pi(\mathbf{z})\left|\det\frac{d\mathbf{z}}{d\mathbf{x}}\right|=\pi(f^{-1}(\mathbf{x}))\left|\det\frac{df^{-1}}{d\mathbf{x}}\right|
$$
其中 $\det$ 是雅可比矩阵的行列式，表示局部体积的缩放因子。对流模型来说，我们更常用对数形式（把乘法变加法），便于数值稳定与优化。

于是离散流模型的对数密度可以写成：

$$
\log p_{K} ( z_{K} )=\log p_{0} ( z_{0} )-\sum_{i=1}^{K} \log \left| \det \frac{d f_{i} ( z_{i-1} )} {d z_{i-1}} \right|
$$
这条式子里最“贵”的部分就是雅可比行列式：在高维下直接算 $\det(\cdot)$ 往往很耗时，这也推动了连续化版本的提出。

### 连续型归一化流
连续型归一化流（CNF）的关键在于：把离散的可逆层换成一个连续时间 ODE。令随机变量随时间演化：

$$
\frac{d \mathbf{z}(t)}{d t} = f(\mathbf{z}(t), t,\theta)
$$

此时对应的密度也会随时间变化。一个重要结论（instantaneous change of variables）给出了对数密度的瞬时变化率：

$$
\frac{d}{d t}\log q(\mathbf{z}(t)) = -\mathrm{Tr}\left(\frac{\partial f}{\partial \mathbf{z}(t)}\right)
$$

把它从 $t_0$ 积分到 $t_1$，就得到：

![](https://raw.githubusercontent.com/powerli2002/project-img/main/myblog/20250329140847801.png)

$$
\log q(\mathbf{z}(t_{1}))=\log q(\mathbf{z}(t_{0}))-\int_{t_{0}}^{t_{1}} \mathrm{Tr}\left(\frac{\partial f}{\partial\mathbf{z}( t )}\right) d t
$$

这里出现“迹（trace）”并不神秘：它可以看成是离散情形里 $\log\det$ 在连续极限下的一阶近似（$\log\det(I+\varepsilon A)\approx \varepsilon\,\mathrm{Tr}(A)$）。直观上，迹刻画的是向量场在局部对体积的“发散/收缩”趋势。

训练时我们仍然最大化数据在模型下的对数似然（或最小化负对数似然），只是密度的变化从“层层累加 $\log|\det|$”变成了“沿时间积分迹”。随机变量 $\mathbf{z}(t_0)$ 及其分布会被连续动力系统推送到 $\mathbf{z}(t_1)$：
![|430](https://raw.githubusercontent.com/powerli2002/project-img/main/myblog/20250329140847802.png)

## Reference

- [硬核NeruIPS 2018最佳论文，一个神经了的常微分方程](https://mp.weixin.qq.com/s/ZEIsyV-0aTvYn6K8GyANPA)
- Chen, Rubanova, Bettencourt, Duvenaud. *Neural Ordinary Differential Equations*. NeurIPS 2018.
